# ğŸ“š Question Generation System (Offline, Local Models)

An elegant, extendable, and offline-friendly question generation system that uses locally hosted language models (e.g., LLaMA, DeepSeek) to generate high-quality questions from lesson content or example exam files.

---

## ğŸš€ Features

- ğŸ“„ Supports two input types:
  - Markdown lesson content (`.md`)
  - Example questions in LaTeX format (`.tex`)
- ğŸ¤– Choose from local models manually â€” or let the system **auto-select** the best model
- ğŸŒ Output language: **Persian** or **English**
- ğŸ“¥ Outputs result as a downloadable `.tex` file
- ğŸ’» Clean, modern UI with particle background and responsive design
- ğŸ“œ Real-time status log with timestamps and detailed process tracking

---

## ğŸ“ Project Structure

```
modern-qg/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ style.css
â”‚       â”œâ”€â”€ script.js
â”‚       â””â”€â”€ particles-config.js
â”œâ”€â”€ models/
â”‚   â””â”€â”€ (your .gguf model files here)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1. Clone the repository

```bash
git clone https://github.com/sirnaser/modern-qg.git
cd modern-qg
```

### 2. Create virtual environment (optional but recommended)

```bash
python -m venv venv
source venv/bin/activate        # Linux/macOS
venv\Scripts\activate           # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Add your models

Place your `.gguf` model files inside the `models/` directory. For example:

```
models/
â”œâ”€â”€ DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf
â””â”€â”€ Llama-3.2-3B-Instruct-Q4_K_M.gguf
```

---

## ğŸ–¥ï¸ Running the App

Start the FastAPI server with:

```bash
uvicorn backend.main:app --reload
```

Then open your browser and go to:

```
http://localhost:8000
```

---

## ğŸ“¤ Inputs

### Markdown Mode
Upload a `.md` file containing raw lesson content.

### TeX Mode
Upload a `.tex` file containing example exam questions.

---

## ğŸ“„ Output Format

The result is generated as a `.tex` file with the following structure:

```latex
\section*{Questions}
(Generated questions)

\newpage

\section*{Answers}
(Answers and references)
```

---

## ğŸ¤– Model Selection

You can either:
- **Manually** select a model from the dropdown, or
- Choose **"Auto-select best model"** to let the system analyze the input and decide which model fits best (using a helper model like DeepSeek).

---

## ğŸ› ï¸ Advanced Notes

- Make sure your system supports local inference. At least **8GB RAM** is recommended.
- The backend uses [`llama-cpp-python`](https://pypi.org/project/llama-cpp-python/) â€” no need to compile llama.cpp manually.
- If you encounter this error:

  ```
  Requested tokens (5303) exceed context window of 4096
  ```

  You may adjust the context size (`n_ctx`) in `utils.py`:

  ```python
  llm = Llama(
      model_path=model_path,
      n_ctx=8192,
      ...
  )
  ```

---

## ğŸ’¬ Status Log

The UI logs all actions, including:

- File uploads
- Model/language changes
- Auto-selection attempts
- Model generation start/complete
- Error handling with timestamp

This helps users know whatâ€™s happening under the hood â€” like a mini debug console.

---

## ğŸ‘¤ Author

Designed and developed by [@sirnaser](https://github.com/sirnaser)  
Made with â¤ï¸ for educational tools and open-source AI

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).
